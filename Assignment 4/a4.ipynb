{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional,Flatten\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from files and save it into lists\n",
    "def load_data(f, data, pos_or_neg, target):\n",
    "    for line in f.readlines():\n",
    "        line_str = \"\"\n",
    "        line = line[1:-2].split(r\", \")\n",
    "        for i in range(len(line)):\n",
    "            line[i] = line[i][1:-1]\n",
    "            line_str += str(line[i]+\" \")\n",
    "        data.append(line_str)\n",
    "        target.append(pos_or_neg)\n",
    "    \n",
    "data = []\n",
    "target = []\n",
    "\n",
    "for file_name in [\"training_pos.csv\",\n",
    "          \"training_neg.csv\",\n",
    "          \"validation_pos.csv\",\n",
    "          \"validation_neg.csv\",\n",
    "          \"test_pos.csv\",\n",
    "          \"test_neg.csv\"]:\n",
    "    f = open(DATA_DIR + file_name,'r')\n",
    "    pos_or_neg = 1 if 'pos' in file_name else 0\n",
    "    load_data(f,data, pos_or_neg, target)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th Percentile Sentence Length: 23\n"
     ]
    }
   ],
   "source": [
    "word_seq = [text_to_word_sequence(sent) for sent in data]\n",
    "# we should be safe using MAX _SENT _LEN as 90th Percentile Sentence Length\n",
    "MAX_SENT_LEN = int(np.percentile([len(seq) for seq in word_seq], 90))\n",
    "print('90th Percentile Sentence Length:', MAX_SENT_LEN)\n",
    "\n",
    "# cut every sentence  according to MAX_SENT_LEN\n",
    "data = [' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq]\n",
    "\n",
    "# Convert the sequence of words to sequnce of indices\n",
    "MAX_VOCAB_SIZE = 80000\n",
    "tokenizer = Tokenizer(MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(data)\n",
    "data = tokenizer.texts_to_sequences(data)\n",
    "data = pad_sequences(data, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into train, validation, test\n",
    "length = len(data)\n",
    "train_data = data[:int(length*0.8)]\n",
    "val_data = data[int(length*0.9):]\n",
    "test_data = data[int(length*0.8):int(length*0.9)]\n",
    "train_target = target[:int(length*0.8)]\n",
    "val_target = target[int(length*0.9):]\n",
    "test_target = target[int(length*0.8):int(length*0.9)]\n",
    "\n",
    "train = list(zip(train_data, train_target))\n",
    "import random\n",
    "random.shuffle(train)\n",
    "train_data[:], train_target[:] = zip(*train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target = np_utils.to_categorical(train_target, 2)\n",
    "test_target =  np_utils.to_categorical(test_target, 2)\n",
    "val_target = np_utils.to_categorical(val_target, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of w2v: 300\n"
     ]
    }
   ],
   "source": [
    "W2V_DIR = 'W2V_DIR'\n",
    "embeddings = Word2Vec.load(DATA_DIR + W2V_DIR)\n",
    "print('Dimension of w2v:', embeddings.vector_size)\n",
    "EMBEDDING_DIM = embeddings.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Create an embedding matrix containing only the word's in our vocabulary\n",
    "# If the word does not have a pre-trained embedding, then randomly initialize the embedding\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
    "\n",
    "for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n",
    "del embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate):\n",
    "    \n",
    "    # Build a sequential model by stacking neural net units \n",
    "    model = Sequential()\n",
    "    # Input layer of embeddings\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                              output_dim=EMBEDDING_DIM,\n",
    "                              weights = [embeddings_matrix], trainable=True, name='word_embedding_layer', \n",
    "                              mask_zero=False,input_length=MAX_SENT_LEN))\n",
    "#     model.add(LSTM(128, return_sequences=False, name='lstm_layer'))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if(dropoutFlag):\n",
    "        model.add(Dropout(dropoutRate))\n",
    "        \n",
    "    if(l2normFlag):\n",
    "        model.add(Dense(units = 30,activation = hidden_actfunc, name = 'hidden_layer',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    else:\n",
    "        model.add(Dense(units = 30,activation = hidden_actfunc, name = 'hidden_layer'))\n",
    "    \n",
    "    if(dropoutFlag):\n",
    "        model.add(Dropout(dropoutRate))\n",
    "    \n",
    "    if(l2normFlag):\n",
    "        model.add(Dense(2, activation='softmax', name='output_layer',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "    else:\n",
    "        model.add(Dense(2, activation='softmax', name='output_layer'))\n",
    "\n",
    "    # Use cross-entropy as the loss function\n",
    "    model.compile(loss='categorical_crossentropy',optimizer ='adam',  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# from IPython.display import Image\n",
    "# plot_model(model, to_file='basic_lstm_classifier.png', show_layer_names=True, show_shapes=True)\n",
    "# Image('basic_lstm_classifier.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # train data\n",
    "# BATCH_SIZE = 128\n",
    "# N_EPOCHS = 10\n",
    "# dropoutRate = 0.5\n",
    "# for hidden_actfunc in [\"relu\", \"sigmoid\", \"tanh\"]:\n",
    "#     for l2normFlag in [True, False]:\n",
    "#         for dropoutFlag in [True, False]:\n",
    "#             print(\"----------------------------------------------------------------------------------\")\n",
    "#             print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "#                       \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "#             for dropoutRate in np.arange(0.4, 1 , 0.2):\n",
    "#                 model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "#                 model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0625 12:20:14.654095 113336 deprecation_wrapper.py:119] From C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0625 12:20:14.682020 113336 deprecation_wrapper.py:119] From C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0625 12:20:14.691023 113336 deprecation_wrapper.py:119] From C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0625 12:20:14.701988 113336 deprecation_wrapper.py:119] From C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0625 12:20:14.702964 113336 deprecation_wrapper.py:119] From C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0625 12:20:17.631944 113336 deprecation.py:506] From C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0625 12:20:17.679821 113336 deprecation_wrapper.py:119] From C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0625 12:20:17.781545 113336 deprecation.py:323] From C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: relu   l2normFlag: True   dropoutFlag: True   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 131s 204us/step - loss: 0.7795 - acc: 0.5416 - val_loss: 0.7189 - val_acc: 0.6640\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.7454 - acc: 0.5826 - val_loss: 0.6916 - val_acc: 0.7140\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.7396 - acc: 0.5992 - val_loss: 0.6745 - val_acc: 0.7333\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 130s 203us/step - loss: 0.7342 - acc: 0.6081 - val_loss: 0.6603 - val_acc: 0.7564\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.7301 - acc: 0.6130 - val_loss: 0.6594 - val_acc: 0.7639\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 130s 203us/step - loss: 0.7268 - acc: 0.6172 - val_loss: 0.6457 - val_acc: 0.7668\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 130s 203us/step - loss: 0.7242 - acc: 0.6206 - val_loss: 0.6478 - val_acc: 0.7703\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 130s 203us/step - loss: 0.7217 - acc: 0.6232 - val_loss: 0.6491 - val_acc: 0.7647\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.7202 - acc: 0.6254 - val_loss: 0.6319 - val_acc: 0.7745\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.7182 - acc: 0.6266 - val_loss: 0.6308 - val_acc: 0.7745\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 76.86%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = relu, l2norm = True, dropout = True\n",
    "hidden_actfunc = \"relu\"\n",
    "l2normFlag = True\n",
    "dropoutFlag = True\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: relu   l2normFlag: True   dropoutFlag: False   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 128s 201us/step - loss: 0.7305 - acc: 0.5818 - val_loss: 0.6240 - val_acc: 0.7260\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 127s 199us/step - loss: 0.6782 - acc: 0.6204 - val_loss: 0.5905 - val_acc: 0.7615\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 127s 199us/step - loss: 0.6706 - acc: 0.6312 - val_loss: 0.5895 - val_acc: 0.7711\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 127s 199us/step - loss: 0.6658 - acc: 0.6364 - val_loss: 0.5726 - val_acc: 0.7792\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 127s 199us/step - loss: 0.6625 - acc: 0.6400 - val_loss: 0.5650 - val_acc: 0.7833\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 128s 199us/step - loss: 0.6598 - acc: 0.6427 - val_loss: 0.5603 - val_acc: 0.7802\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6576 - acc: 0.6452 - val_loss: 0.5659 - val_acc: 0.7823\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.6559 - acc: 0.6474 - val_loss: 0.5620 - val_acc: 0.7856\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.6543 - acc: 0.6494 - val_loss: 0.5480 - val_acc: 0.7840\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.6531 - acc: 0.6509 - val_loss: 0.5650 - val_acc: 0.7840\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 78.33%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = relu, l2norm = True, dropout = False\n",
    "hidden_actfunc = \"relu\"\n",
    "l2normFlag = True\n",
    "dropoutFlag = False\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: relu   l2normFlag: False   dropoutFlag: True   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 123s 192us/step - loss: 0.6717 - acc: 0.5830 - val_loss: 0.5816 - val_acc: 0.7564\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6549 - acc: 0.6155 - val_loss: 0.5626 - val_acc: 0.7722\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6493 - acc: 0.6255 - val_loss: 0.5825 - val_acc: 0.7639\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6452 - acc: 0.6316 - val_loss: 0.5581 - val_acc: 0.7819\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6415 - acc: 0.6361 - val_loss: 0.5496 - val_acc: 0.7843\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6379 - acc: 0.6408 - val_loss: 0.5133 - val_acc: 0.7891\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6341 - acc: 0.6443 - val_loss: 0.5383 - val_acc: 0.7801\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6305 - acc: 0.6480 - val_loss: 0.5270 - val_acc: 0.7844\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6270 - acc: 0.6503 - val_loss: 0.5440 - val_acc: 0.7727\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6233 - acc: 0.6544 - val_loss: 0.5386 - val_acc: 0.7754\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 77.38%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = relu, l2norm = False, dropout = True\n",
    "hidden_actfunc = \"relu\"\n",
    "l2normFlag = False\n",
    "dropoutFlag = True\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: relu   l2normFlag: False   dropoutFlag: False   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 120s 188us/step - loss: 0.6511 - acc: 0.6163 - val_loss: 0.5342 - val_acc: 0.7834\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.6301 - acc: 0.6472 - val_loss: 0.5267 - val_acc: 0.7905\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.6175 - acc: 0.6607 - val_loss: 0.5142 - val_acc: 0.7883\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.6041 - acc: 0.6725 - val_loss: 0.5181 - val_acc: 0.7800\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.5904 - acc: 0.6832 - val_loss: 0.5167 - val_acc: 0.7732\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.5774 - acc: 0.6918 - val_loss: 0.5199 - val_acc: 0.7689\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.5658 - acc: 0.6981 - val_loss: 0.5205 - val_acc: 0.7629\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.5553 - acc: 0.7034 - val_loss: 0.5268 - val_acc: 0.7609\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.5457 - acc: 0.7085 - val_loss: 0.5363 - val_acc: 0.7494\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 119s 187us/step - loss: 0.5372 - acc: 0.7116 - val_loss: 0.5387 - val_acc: 0.7509\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 75.22%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = relu, l2norm = False, dropout = False\n",
    "hidden_actfunc = \"relu\"\n",
    "l2normFlag = False\n",
    "dropoutFlag = False\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: sigmoid   l2normFlag: True   dropoutFlag: True   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 123s 192us/step - loss: 0.7359 - acc: 0.5033 - val_loss: 0.6999 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.7000 - acc: 0.5011 - val_loss: 0.7003 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.7003 - acc: 0.5030 - val_loss: 0.7016 - val_acc: 0.5145\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.7029 - acc: 0.5089 - val_loss: 0.7070 - val_acc: 0.5284\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.7200 - acc: 0.5501 - val_loss: 0.7070 - val_acc: 0.6833\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.7268 - acc: 0.5788 - val_loss: 0.6925 - val_acc: 0.7146\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.7260 - acc: 0.5922 - val_loss: 0.6787 - val_acc: 0.7252\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.7245 - acc: 0.6002 - val_loss: 0.6744 - val_acc: 0.7444\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.7225 - acc: 0.6058 - val_loss: 0.6714 - val_acc: 0.7396\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.7208 - acc: 0.6105 - val_loss: 0.6606 - val_acc: 0.7433\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 74.14%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = sigmoid, l2norm = True, dropout = True\n",
    "hidden_actfunc = \"sigmoid\" \n",
    "l2normFlag = True\n",
    "dropoutFlag = True\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: sigmoid   l2normFlag: True   dropoutFlag: False   dropoutRate 0.5Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 121s 188us/step - loss: 0.7194 - acc: 0.5029 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6940 - acc: 0.5008 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6939 - acc: 0.5007 - val_loss: 0.6945 - val_acc: 0.5397\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6937 - acc: 0.5002 - val_loss: 0.6934 - val_acc: 0.6782\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6935 - acc: 0.5401 - val_loss: 0.6933 - val_acc: 0.7354\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6934 - acc: 0.5929 - val_loss: 0.6074 - val_acc: 0.7358\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6933 - acc: 0.6234 - val_loss: 0.5760 - val_acc: 0.7691\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6933 - acc: 0.6340 - val_loss: 0.5741 - val_acc: 0.7724\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6933 - acc: 0.6396 - val_loss: 0.5625 - val_acc: 0.7778\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 120s 187us/step - loss: 0.6933 - acc: 0.6443 - val_loss: 0.5622 - val_acc: 0.7796\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 78.08%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = sigmoid, l2norm = True, dropout = False\n",
    "hidden_actfunc = \"sigmoid\"\n",
    "l2normFlag = True\n",
    "dropoutFlag = False\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: sigmoid   l2normFlag: False   dropoutFlag: True   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 123s 192us/step - loss: 0.6655 - acc: 0.5938 - val_loss: 0.5595 - val_acc: 0.7690\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.6477 - acc: 0.6240 - val_loss: 0.5368 - val_acc: 0.7779\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.6418 - acc: 0.6330 - val_loss: 0.5381 - val_acc: 0.7887\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.6371 - acc: 0.6405 - val_loss: 0.5258 - val_acc: 0.7901\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.6336 - acc: 0.6443 - val_loss: 0.5293 - val_acc: 0.7935\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.6311 - acc: 0.6473 - val_loss: 0.5252 - val_acc: 0.7916\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.6284 - acc: 0.6504 - val_loss: 0.5248 - val_acc: 0.7924\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.6262 - acc: 0.6533 - val_loss: 0.5080 - val_acc: 0.7931\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.6239 - acc: 0.6558 - val_loss: 0.5055 - val_acc: 0.7935\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 122s 190us/step - loss: 0.6220 - acc: 0.6580 - val_loss: 0.5058 - val_acc: 0.7925\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 79.44%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = sigmoid, l2norm = False, dropout = True\n",
    "hidden_actfunc = \"sigmoid\"\n",
    "l2normFlag = False\n",
    "dropoutFlag = True\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: sigmoid   l2normFlag: False   dropoutFlag: False   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 120s 188us/step - loss: 0.6509 - acc: 0.6166 - val_loss: 0.5399 - val_acc: 0.7765\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.6301 - acc: 0.6468 - val_loss: 0.5184 - val_acc: 0.7841\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.6192 - acc: 0.6591 - val_loss: 0.5174 - val_acc: 0.7887\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.6091 - acc: 0.6690 - val_loss: 0.5077 - val_acc: 0.7855\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.5989 - acc: 0.6780 - val_loss: 0.5158 - val_acc: 0.7792\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.5887 - acc: 0.6863 - val_loss: 0.5116 - val_acc: 0.7751\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.5786 - acc: 0.6934 - val_loss: 0.5154 - val_acc: 0.7692\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.5688 - acc: 0.6997 - val_loss: 0.5226 - val_acc: 0.7652\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.5599 - acc: 0.7046 - val_loss: 0.5223 - val_acc: 0.7595\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 119s 186us/step - loss: 0.5516 - acc: 0.7089 - val_loss: 0.5329 - val_acc: 0.7532\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 75.52%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = sigmoid, l2norm = False, dropout = False\n",
    "hidden_actfunc = \"sigmoid\"\n",
    "l2normFlag = False\n",
    "dropoutFlag = False\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: sigmoid   l2normFlag: True   dropoutFlag: True   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.7307 - acc: 0.5031 - val_loss: 0.6988 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 121s 189us/step - loss: 0.6999 - acc: 0.5010 - val_loss: 0.7013 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 123s 192us/step - loss: 0.7000 - acc: 0.5019 - val_loss: 0.7002 - val_acc: 0.5117\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 125s 195us/step - loss: 0.7027 - acc: 0.5085 - val_loss: 0.6984 - val_acc: 0.6201\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 124s 195us/step - loss: 0.7191 - acc: 0.5481 - val_loss: 0.7060 - val_acc: 0.6666\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 128s 200us/step - loss: 0.7268 - acc: 0.5780 - val_loss: 0.6975 - val_acc: 0.7142\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 131s 204us/step - loss: 0.7261 - acc: 0.5914 - val_loss: 0.6838 - val_acc: 0.7285\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.7243 - acc: 0.5994 - val_loss: 0.6692 - val_acc: 0.7410\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 129s 201us/step - loss: 0.7228 - acc: 0.6055 - val_loss: 0.6675 - val_acc: 0.7457\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 130s 204us/step - loss: 0.7213 - acc: 0.6102 - val_loss: 0.6573 - val_acc: 0.7483\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 74.62%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = tanh, l2norm = True, dropout = True\n",
    "hidden_actfunc = \"sigmoid\"\n",
    "l2normFlag = True\n",
    "dropoutFlag = True\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: tanh   l2normFlag: True   dropoutFlag: False   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 128s 200us/step - loss: 0.7583 - acc: 0.5656 - val_loss: 0.6652 - val_acc: 0.7044\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 128s 199us/step - loss: 0.7078 - acc: 0.6079 - val_loss: 0.6407 - val_acc: 0.7316\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 131s 205us/step - loss: 0.7031 - acc: 0.6206 - val_loss: 0.6365 - val_acc: 0.7447\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.6979 - acc: 0.6281 - val_loss: 0.6186 - val_acc: 0.7637\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.6940 - acc: 0.6330 - val_loss: 0.5977 - val_acc: 0.7697\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 130s 202us/step - loss: 0.6910 - acc: 0.6370 - val_loss: 0.6087 - val_acc: 0.7799\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.6883 - acc: 0.6390 - val_loss: 0.5953 - val_acc: 0.7764\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.6866 - acc: 0.6411 - val_loss: 0.5900 - val_acc: 0.7794\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.6843 - acc: 0.6431 - val_loss: 0.6010 - val_acc: 0.7830\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.6828 - acc: 0.6450 - val_loss: 0.5938 - val_acc: 0.7761\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 77.58%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = tanh, l2norm = True, dropout = False\n",
    "hidden_actfunc = \"tanh\"\n",
    "l2normFlag = True\n",
    "dropoutFlag = False\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: tanh   l2normFlag: False   dropoutFlag: True   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 130s 203us/step - loss: 0.6764 - acc: 0.5847 - val_loss: 0.5582 - val_acc: 0.7603\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 131s 205us/step - loss: 0.6514 - acc: 0.6180 - val_loss: 0.5410 - val_acc: 0.7780\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 131s 205us/step - loss: 0.6444 - acc: 0.6293 - val_loss: 0.5231 - val_acc: 0.7845\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 131s 205us/step - loss: 0.6406 - acc: 0.6353 - val_loss: 0.5256 - val_acc: 0.7904\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 131s 205us/step - loss: 0.6376 - acc: 0.6396 - val_loss: 0.5248 - val_acc: 0.7915\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 131s 205us/step - loss: 0.6353 - acc: 0.6429 - val_loss: 0.5182 - val_acc: 0.7929\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 131s 205us/step - loss: 0.6326 - acc: 0.6467 - val_loss: 0.5159 - val_acc: 0.7943\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 131s 205us/step - loss: 0.6312 - acc: 0.6489 - val_loss: 0.5213 - val_acc: 0.7946\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 131s 205us/step - loss: 0.6294 - acc: 0.6509 - val_loss: 0.5217 - val_acc: 0.7965\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 123s 192us/step - loss: 0.6279 - acc: 0.6533 - val_loss: 0.5091 - val_acc: 0.7964\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 79.75%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = tanh, l2norm = False, dropout = True\n",
    "hidden_actfunc = \"tanh\"\n",
    "l2normFlag = False\n",
    "dropoutFlag = True\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "hidden_actfunc: tanh   l2normFlag: False   dropoutFlag: False   dropoutRate 0.5\n",
      "Train on 640000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "640000/640000 [==============================] - 124s 194us/step - loss: 0.6568 - acc: 0.6089 - val_loss: 0.5386 - val_acc: 0.7688\n",
      "Epoch 2/10\n",
      "640000/640000 [==============================] - 123s 192us/step - loss: 0.6357 - acc: 0.6392 - val_loss: 0.5306 - val_acc: 0.7786\n",
      "Epoch 3/10\n",
      "640000/640000 [==============================] - 122s 191us/step - loss: 0.6270 - acc: 0.6500 - val_loss: 0.5222 - val_acc: 0.7829\n",
      "Epoch 4/10\n",
      "640000/640000 [==============================] - 123s 193us/step - loss: 0.6206 - acc: 0.6579 - val_loss: 0.5210 - val_acc: 0.7844\n",
      "Epoch 5/10\n",
      "640000/640000 [==============================] - 134s 210us/step - loss: 0.6146 - acc: 0.6636 - val_loss: 0.5155 - val_acc: 0.7808\n",
      "Epoch 6/10\n",
      "640000/640000 [==============================] - 135s 211us/step - loss: 0.6084 - acc: 0.6699 - val_loss: 0.5179 - val_acc: 0.7786\n",
      "Epoch 7/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.6025 - acc: 0.6760 - val_loss: 0.5183 - val_acc: 0.7778\n",
      "Epoch 8/10\n",
      "640000/640000 [==============================] - 129s 201us/step - loss: 0.5963 - acc: 0.6811 - val_loss: 0.5136 - val_acc: 0.7729\n",
      "Epoch 9/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.5905 - acc: 0.6854 - val_loss: 0.5143 - val_acc: 0.7724\n",
      "Epoch 10/10\n",
      "640000/640000 [==============================] - 129s 202us/step - loss: 0.5842 - acc: 0.6903 - val_loss: 0.5130 - val_acc: 0.7679\n",
      "80000/80000 [==============================] - 3s 35us/step\n",
      "acc: 76.59%\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = tanh, l2norm = False, dropout = False\n",
    "hidden_actfunc = \"tanh\"\n",
    "l2normFlag = False\n",
    "dropoutFlag = False\n",
    "dropoutRate = 0.5\n",
    "model = sequential_model (hidden_actfunc, l2normFlag, dropoutFlag, dropoutRate)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"hidden_actfunc: \"+hidden_actfunc, \"  l2normFlag: \" + str(l2normFlag), \n",
    "      \"  dropoutFlag: \"+str(dropoutFlag), \"  dropoutRate \" + str(dropoutRate))\n",
    "model.fit(train_data, train_target,batch_size=BATCH_SIZE,epochs=N_EPOCHS, validation_data =(val_data, val_target))\n",
    "scores = model.evaluate(test_data, test_target)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
